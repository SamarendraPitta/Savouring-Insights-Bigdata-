{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf671250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import spacy\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')  # Load spaCy's small English model\n",
    "\n",
    "# Step 1: Extract Nouns from POS Tags\n",
    "def extract_nouns(pos_tags):\n",
    "    \"\"\"Extract relevant nouns (NN, NNS, NNP) with filtering rules.\"\"\"\n",
    "    nouns = []\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in {'NN', 'NNS'}:\n",
    "            nouns.append(word)\n",
    "        elif tag == 'NNP':  # Handle proper nouns\n",
    "            nouns.append(word.lower())\n",
    "    return nouns\n",
    "\n",
    "# Step 2: Frequent Feature Extraction\n",
    "def apriori_frequent_itemsets(baskets, min_support):\n",
    "    \"\"\"Extract frequent itemsets using the Apriori algorithm.\"\"\"\n",
    "    counter = Counter()\n",
    "    for basket in baskets:\n",
    "        for size in range(1, 4):  # Single, two-word, three-word itemsets\n",
    "            for itemset in combinations(basket, size):\n",
    "                counter[itemset] += 1\n",
    "    return {item: count for item, count in counter.items() if count >= min_support}\n",
    "\n",
    "# Step 3: Get Top Frequent Features\n",
    "def get_top_frequent_features(itemsets, top_n=5):\n",
    "    \"\"\"Get the top N most frequent features from itemsets.\"\"\"\n",
    "    sorted_itemsets = sorted(itemsets.items(), key=lambda x: x[1], reverse=True)\n",
    "    return dict(sorted_itemsets[:top_n])\n",
    "\n",
    "# Step 4: Extract Descriptors\n",
    "def extract_descriptors(sentences, frequent_features):\n",
    "    \"\"\"Extract descriptors for features based on dependency parsing.\"\"\"\n",
    "    freq_features = defaultdict(lambda: {\"count\": 0, \"examples\": [], \"descriptors\": set()})\n",
    "\n",
    "    for i, (sentence_id, words, pos_tags, sentence) in enumerate(sentences):\n",
    "        doc = nlp(sentence)  # Parse the sentence using spaCy\n",
    "        for token in doc:\n",
    "            # Look for descriptors related to features (adjectives or subjects)\n",
    "            if token.dep_ in ('amod', 'nsubj', 'advmod', 'compound'):\n",
    "                opinion_word = token.text.lower()  # This is the descriptor word\n",
    "                feature_word = token.head.text  # This is the feature word\n",
    "\n",
    "                # Match feature_word to frequent features\n",
    "                matched = False\n",
    "                for feature in frequent_features:\n",
    "                    feature_lower = [f.lower() for f in feature]\n",
    "                    if feature_word.lower() in feature_lower:\n",
    "                        matched = True\n",
    "                        break\n",
    "\n",
    "                # If the feature is matched, add the descriptor\n",
    "                if matched:\n",
    "                    freq_features[feature[0]][\"descriptors\"].add(opinion_word)\n",
    "                    freq_features[feature[0]][\"count\"] += 1\n",
    "                    freq_features[feature[0]][\"examples\"].append(\" \".join([t.text for t in doc]))\n",
    "\n",
    "            # Capture multi-word phrases (e.g., \"very delicious\")\n",
    "            if token.dep_ == 'advmod' and token.head.dep_ == 'amod':\n",
    "                descriptor_phrase = f\"{token.text} {token.head.text}\".lower()\n",
    "                feature_word = token.head.head.text\n",
    "                for feature in frequent_features:\n",
    "                    feature_lower = [f.lower() for f in feature]\n",
    "                    if feature_word.lower() in feature_lower:\n",
    "                        freq_features[feature[0]][\"descriptors\"].add(descriptor_phrase)\n",
    "                        freq_features[feature[0]][\"count\"] += 1\n",
    "                        freq_features[feature[0]][\"examples\"].append(\" \".join([t.text for t in doc]))\n",
    "                        break\n",
    "\n",
    "    return infrequent_features\n",
    "\n",
    "# Step 5: Filter Descriptors and Features\n",
    "def clean_descriptors_and_features(freq_features):\n",
    "    \"\"\"Clean descriptors and features by removing stopwords, single letters, and nouns.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    additional_stopwords = {\"a\", \"an\", \"the\", \"of\", \"in\", \"on\", \"for\", \"to\", \"with\", \"by\", \"at\", \"as\", \"is\", \"it\"}\n",
    "    stop_words.update(additional_stopwords)\n",
    "\n",
    "    cleaned_features = defaultdict(lambda: {\"count\": 0, \"examples\": [], \"descriptors\": set()})\n",
    "    \n",
    "    for feature, details in freq_features.items():\n",
    "        # Remove nouns from descriptors\n",
    "        filtered_descriptors = {\n",
    "            word for word in details[\"descriptors\"]\n",
    "            if word not in stop_words and len(word) > 1 and word.isalpha()  # Remove stopwords, single letters, non-alphabetic words\n",
    "        }\n",
    "\n",
    "        # Perform POS tagging on descriptors to filter out nouns\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        final_descriptors = set()\n",
    "        for desc in filtered_descriptors:\n",
    "            pos_tagged = pos_tag([desc])\n",
    "            if pos_tagged[0][1] not in {'NN', 'NNS', 'NNP', 'NNPS'}:  # Exclude nouns\n",
    "                final_descriptors.add(lemmatizer.lemmatize(desc))\n",
    "\n",
    "        if final_descriptors:  # Only include features with valid descriptors\n",
    "            cleaned_features[feature][\"descriptors\"] = final_descriptors\n",
    "            cleaned_features[feature][\"count\"] = details[\"count\"]\n",
    "            cleaned_features[feature][\"examples\"] = details[\"examples\"]\n",
    "    \n",
    "    return cleaned_features\n",
    "\n",
    "# Step 6: Combine Results with Filtering\n",
    "def process_reviews(preprocessed_data, min_support=2, top_n=20):\n",
    "    \"\"\"End-to-end processing to extract features and clean descriptors.\"\"\"\n",
    "    baskets = [extract_nouns(pos_tags) for _, _, pos_tags, _ in preprocessed_data]\n",
    "    frequent_itemsets = apriori_frequent_itemsets(baskets, min_support)\n",
    "    top_features = get_top_frequent_features(frequent_itemsets, top_n)\n",
    "    freq_features = extract_descriptors(preprocessed_data, top_features)\n",
    "    cleaned_features = clean_descriptors_and_features(freq_features)\n",
    "    return top_features, cleaned_features\n",
    "\n",
    "# Step 7: Update MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")  # Update with your MongoDB connection details\n",
    "db = client[\"Project\"]  # Replace with your database name\n",
    "collection = db[\"pizza_business_preprocess_reviews\"]  # Replace with your collection name\n",
    "\n",
    "for document in collection.find():  # Retrieve all documents\n",
    "    preprocessed_data = []\n",
    "    if \"preprocessed_data\" in document:\n",
    "        preprocessed_data.extend(document[\"preprocessed_data\"])  # Accumulate preprocessed data\n",
    "        business_id = document.get(\"business_id\", \"Unknown\")  # Get the business_id (replace with actual field if different)\n",
    "\n",
    "    if preprocessed_data:\n",
    "        # Process reviews and clean features\n",
    "        frequent_features, cleaned_features = process_reviews(preprocessed_data)\n",
    "\n",
    "        # Prepare cleaned features with descriptors\n",
    "        extracted_features = []\n",
    "        for feature, descriptors in cleaned_features.items():\n",
    "            extracted_features.append({\n",
    "                \"feature\": feature,\n",
    "                \"descriptors\": list(descriptors[\"descriptors\"])\n",
    "            })\n",
    "        \n",
    "        # Push the cleaned features to the collection\n",
    "        collection.update_one(\n",
    "            {\"business_id\": business_id}, \n",
    "            {\"$set\": {\"extracted_features\": extracted_features}},\n",
    "            upsert=True  # If document doesn't exist, it will be created\n",
    "        )\n",
    "        print(business_id)\n",
    "\n",
    "    else:\n",
    "        print(\"No data found in the collection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
